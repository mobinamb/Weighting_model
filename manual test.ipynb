{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "import cv2\n",
    "from tqdm.notebook import tqdm\n",
    "import cv2 \n",
    "import torch\n",
    "import argparse\n",
    "import os\n",
    "from torchvision import transforms\n",
    "import torch.nn as nn\n",
    "import cv2\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "import random\n",
    "from datetime import datetime\n",
    "import xlsxwriter\n",
    "from sklearn.preprocessing import StandardScaler    \n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "#compensate for data imbalance\n",
    "from sklearn.utils import shuffle\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "meanDataset =[0.4415734549019608,0.388225627197976,0.3462771010120177]\n",
    "stdDataset =[0.2711501212166684,0.24437697540137235,0.2350548283233844]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#the function you need to use to remove the classifier layer\n",
    "class Identity(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Identity, self).__init__()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return x\n",
    "\n",
    "def init_weights(m):\n",
    "    if isinstance(m, nn.Conv2d):\n",
    "        torch.nn.init.kaiming_normal_(m.weight)\n",
    "\n",
    "def ResizedTensor(image, size): # the function to resize the images to the desired size\n",
    "    dim=size\n",
    "    resized = cv2.resize(image, dim, interpolation =  cv2.INTER_LINEAR )\n",
    "    #print(resized.shape)\n",
    "    return resized\n",
    "\n",
    "def CenterCropTensor(image, size): # the function to to do center crop\n",
    "\n",
    "    image_width, image_height, channel = image.shape\n",
    "\n",
    "    #image_width, image_height = _get_image_size(img)\n",
    "    crop_height, crop_width = size\n",
    "\n",
    "    # crop_top = int(round((image_height - crop_height) / 2.))\n",
    "    # Result can be different between python func and scripted func\n",
    "    # Temporary workaround:\n",
    "    crop_top = int((image_height - crop_height + 1) * 0.5)\n",
    "    # crop_left = int(round((image_width - crop_width) / 2.))\n",
    "    # Result can be different between python func and scripted func\n",
    "    # Temporary workaround:\n",
    "    crop_left = int((image_width - crop_width + 1) * 0.5)\n",
    "    img=image[ crop_left: crop_left+crop_width,crop_top:crop_top+crop_height,:];\n",
    "    return img #crop(img, crop_top, crop_left, crop_height, crop_width)\n",
    "\n",
    "class BinaryClassification(nn.Module):\n",
    "        def __init__(self):\n",
    "            super(BinaryClassification, self).__init__()\n",
    "            # Number of input features is 12.\n",
    "            self.layer_1 = nn.Linear(4096, 64) \n",
    "            self.layer_2 = nn.Linear(64, 64)\n",
    "            self.layer_out = nn.Linear(64, 1) \n",
    "            \n",
    "            self.relu = nn.ReLU()\n",
    "            self.dropout = nn.Dropout(p=0.1)\n",
    "            self.batchnorm1 = nn.BatchNorm1d(64)\n",
    "            self.batchnorm2 = nn.BatchNorm1d(64)\n",
    "            \n",
    "        def forward(self, inputs):\n",
    "            x = self.relu(self.layer_1(inputs)) \n",
    "            x = self.batchnorm1(x)\n",
    "            x = self.relu(self.layer_2(x))\n",
    "            x = self.batchnorm2(x)\n",
    "            #x = self.dropout(x)\n",
    "            x = self.layer_out(x) \n",
    "            x=torch.sigmoid(x)   \n",
    "            return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#step 1 preprocess the 2 images\n",
    "\n",
    "def image_preprocess(path):\n",
    "    image=cv2.imread(path)\n",
    "    masked  =  image / 255 \n",
    "    for ch in range (0,3):\n",
    "        masked[:,:,ch]=(masked[:,:,ch]-meanDataset[ch])/stdDataset[ch]  \n",
    "    masked=ResizedTensor(masked,(256,256))\n",
    "    masked=CenterCropTensor(masked,(224,224))\n",
    "    masked=torch.tensor(masked).permute(2, 0, 1).unsqueeze(0).to(device, dtype=torch.float)  #this part move the image from CPU to GPU\n",
    "    return masked\n",
    "#step 2 calculate the feature maps of the two images by applying model1 (feature extractor which was trained on the train dataset)\n",
    "#the function you need to use to remove the classifier layer\n",
    "class Identity(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Identity, self).__init__()\n",
    "    def forward(self, x):\n",
    "        return x\n",
    "        \n",
    "def extract_features(classifier_path,image):\n",
    "    model=torch.load(classifier_path).eval()\n",
    "    model.classifier[6]=Identity()\n",
    "    model.fc=Identity()\n",
    "    model.to(device).eval() #put the model on GPU\n",
    "    f=model(image).cpu().detach().numpy()\n",
    "    f_norm=f/np.sqrt(np.dot(f,np.transpose(f)))\n",
    "    return f_norm\n",
    "    \n",
    "#step 3 find the error between the feature maps\n",
    "def error(f1,f2):\n",
    "    A=f1-f2\n",
    "    return A[0]\n",
    "#step 4 predict if the error relates to the same/ different identitites by applying the classification model on the error vector (which was trained on test dataset)\n",
    "def same_different(unmasked_image_path,masked_image_path,feature_extractor_path,classifier_path):\n",
    "    unmasked=image_preprocess(unmasked_image_path)\n",
    "    masked=image_preprocess(masked_image_path)\n",
    "    f_unmasked=extract_features(feature_extractor_path,unmasked)\n",
    "    f_masked=extract_features(feature_extractor_path,masked)\n",
    "    er=error(f_unmasked,f_masked)\n",
    "\n",
    "    dic={}\n",
    "    for kk in range(len(er)):\n",
    "        dic[str(kk+1)]=er[kk]\n",
    "    dict={'1':dic}\n",
    "\n",
    "    ddf=pd.DataFrame(dict).transpose()\n",
    "    scaler = pickle.load(open('scaler.pkl', 'rb'))\n",
    "    X_test = scaler.transform(ddf) \n",
    "\n",
    "    ## test data    \n",
    "    class TestData(Dataset):\n",
    "        \n",
    "        def __init__(self, X_data):\n",
    "            self.X_data = X_data\n",
    "            \n",
    "        def __getitem__(self, index):\n",
    "            return self.X_data[index]\n",
    "            \n",
    "        def __len__ (self):\n",
    "            return len(self.X_data)\n",
    "\n",
    "    \n",
    "\n",
    "    test_data = TestData(torch.FloatTensor(X_test))\n",
    "    test_loader = DataLoader(dataset=test_data, batch_size=1)\n",
    "\n",
    "    classifier=torch.load(classifier_path)\n",
    "    classifier.to(device).eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "            for X_batch in test_loader:\n",
    "                X_batch = X_batch.to(device)\n",
    "                y_test_pred = classifier(X_batch)\n",
    "                #y_test_pred = torch.sigmoid(y_test_pred)\n",
    "                y_pred_tag = torch.round(y_test_pred)\n",
    "                if y_pred_tag.item()==1.0:\n",
    "                    return 'same'\n",
    "                else:\n",
    "                    return 'different'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'same'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unmasked_image_path='Amelia_Vega_0002.jpg'\n",
    "masked_image_path='Wen_Jiabao_0002.jpg_mask.jpg'#'Amelia_Vega_0001.jpg_mask.jpg'\n",
    "feature_extractor_path='modelWithBestValidationAcc_unmasked_100epochs_v2.h5'\n",
    "classifier_path='classification_model.h5'\n",
    "same_different(unmasked_image_path,masked_image_path,feature_extractor_path,classifier_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "0d17f44f4d285859fbe37bc4ef85a3a55bc66f314eb39f44e95d2173affd3a8f"
  },
  "kernelspec": {
   "display_name": "Python 3.6.15",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.15"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
